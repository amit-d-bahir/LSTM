{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Part 1 -> Data Preprocessing**\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the training set\n",
    "#     We train the RNN only on our test set, its like the test set doens't exist for RNN\n",
    "df_train = pd.read_csv('Recurrent_Neural_Networks/Google_Stock_Price_Train.csv')\n",
    "\n",
    "#     Contains the input data of the neural network\n",
    "training_set = df_train.iloc[:, 1:2].values  # We add 2 just to make it a numpy array and not a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/10/2012</td>\n",
       "      <td>313.70</td>\n",
       "      <td>315.72</td>\n",
       "      <td>307.30</td>\n",
       "      <td>621.43</td>\n",
       "      <td>8,824,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/11/2012</td>\n",
       "      <td>310.59</td>\n",
       "      <td>313.52</td>\n",
       "      <td>309.40</td>\n",
       "      <td>624.25</td>\n",
       "      <td>4,817,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/12/2012</td>\n",
       "      <td>314.43</td>\n",
       "      <td>315.26</td>\n",
       "      <td>312.08</td>\n",
       "      <td>627.92</td>\n",
       "      <td>3,764,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/13/2012</td>\n",
       "      <td>311.96</td>\n",
       "      <td>312.30</td>\n",
       "      <td>309.37</td>\n",
       "      <td>623.28</td>\n",
       "      <td>4,631,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1/17/2012</td>\n",
       "      <td>314.81</td>\n",
       "      <td>314.81</td>\n",
       "      <td>311.67</td>\n",
       "      <td>626.86</td>\n",
       "      <td>3,832,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1/18/2012</td>\n",
       "      <td>312.14</td>\n",
       "      <td>315.82</td>\n",
       "      <td>309.90</td>\n",
       "      <td>631.18</td>\n",
       "      <td>5,544,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1/19/2012</td>\n",
       "      <td>319.30</td>\n",
       "      <td>319.30</td>\n",
       "      <td>314.55</td>\n",
       "      <td>637.82</td>\n",
       "      <td>12,657,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/20/2012</td>\n",
       "      <td>294.16</td>\n",
       "      <td>294.40</td>\n",
       "      <td>289.76</td>\n",
       "      <td>584.39</td>\n",
       "      <td>21,231,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/23/2012</td>\n",
       "      <td>291.91</td>\n",
       "      <td>293.23</td>\n",
       "      <td>290.49</td>\n",
       "      <td>583.92</td>\n",
       "      <td>6,851,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1/24/2012</td>\n",
       "      <td>292.07</td>\n",
       "      <td>292.74</td>\n",
       "      <td>287.92</td>\n",
       "      <td>579.34</td>\n",
       "      <td>6,134,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1/25/2012</td>\n",
       "      <td>287.68</td>\n",
       "      <td>288.27</td>\n",
       "      <td>282.13</td>\n",
       "      <td>567.93</td>\n",
       "      <td>10,012,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1/26/2012</td>\n",
       "      <td>284.92</td>\n",
       "      <td>286.17</td>\n",
       "      <td>281.22</td>\n",
       "      <td>566.54</td>\n",
       "      <td>6,476,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1/27/2012</td>\n",
       "      <td>284.32</td>\n",
       "      <td>289.08</td>\n",
       "      <td>283.60</td>\n",
       "      <td>578.39</td>\n",
       "      <td>7,262,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1/30/2012</td>\n",
       "      <td>287.95</td>\n",
       "      <td>288.92</td>\n",
       "      <td>285.63</td>\n",
       "      <td>576.11</td>\n",
       "      <td>4,678,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1/31/2012</td>\n",
       "      <td>290.41</td>\n",
       "      <td>290.91</td>\n",
       "      <td>286.50</td>\n",
       "      <td>578.52</td>\n",
       "      <td>4,300,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2/1/2012</td>\n",
       "      <td>291.38</td>\n",
       "      <td>291.66</td>\n",
       "      <td>288.49</td>\n",
       "      <td>579.24</td>\n",
       "      <td>4,658,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2/2/2012</td>\n",
       "      <td>291.34</td>\n",
       "      <td>292.11</td>\n",
       "      <td>289.95</td>\n",
       "      <td>583.51</td>\n",
       "      <td>4,847,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2/3/2012</td>\n",
       "      <td>294.23</td>\n",
       "      <td>297.42</td>\n",
       "      <td>292.93</td>\n",
       "      <td>594.7</td>\n",
       "      <td>6,360,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2/6/2012</td>\n",
       "      <td>296.39</td>\n",
       "      <td>304.27</td>\n",
       "      <td>295.90</td>\n",
       "      <td>607.42</td>\n",
       "      <td>7,386,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2/7/2012</td>\n",
       "      <td>302.44</td>\n",
       "      <td>303.56</td>\n",
       "      <td>300.75</td>\n",
       "      <td>605.11</td>\n",
       "      <td>4,199,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2/8/2012</td>\n",
       "      <td>303.18</td>\n",
       "      <td>304.53</td>\n",
       "      <td>301.24</td>\n",
       "      <td>608.18</td>\n",
       "      <td>3,686,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2/9/2012</td>\n",
       "      <td>304.87</td>\n",
       "      <td>306.10</td>\n",
       "      <td>303.36</td>\n",
       "      <td>609.79</td>\n",
       "      <td>4,546,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2/10/2012</td>\n",
       "      <td>302.81</td>\n",
       "      <td>302.93</td>\n",
       "      <td>300.87</td>\n",
       "      <td>604.25</td>\n",
       "      <td>4,667,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2/13/2012</td>\n",
       "      <td>304.11</td>\n",
       "      <td>305.77</td>\n",
       "      <td>303.87</td>\n",
       "      <td>610.52</td>\n",
       "      <td>3,646,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2/14/2012</td>\n",
       "      <td>304.63</td>\n",
       "      <td>304.86</td>\n",
       "      <td>301.25</td>\n",
       "      <td>608.09</td>\n",
       "      <td>3,620,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>11/17/2016</td>\n",
       "      <td>766.92</td>\n",
       "      <td>772.70</td>\n",
       "      <td>764.23</td>\n",
       "      <td>771.23</td>\n",
       "      <td>1,304,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>11/18/2016</td>\n",
       "      <td>771.37</td>\n",
       "      <td>775.00</td>\n",
       "      <td>760.00</td>\n",
       "      <td>760.54</td>\n",
       "      <td>1,547,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>11/21/2016</td>\n",
       "      <td>762.61</td>\n",
       "      <td>769.70</td>\n",
       "      <td>760.60</td>\n",
       "      <td>769.2</td>\n",
       "      <td>1,330,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>11/22/2016</td>\n",
       "      <td>772.63</td>\n",
       "      <td>776.96</td>\n",
       "      <td>767.00</td>\n",
       "      <td>768.27</td>\n",
       "      <td>1,593,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>11/23/2016</td>\n",
       "      <td>767.73</td>\n",
       "      <td>768.28</td>\n",
       "      <td>755.25</td>\n",
       "      <td>760.99</td>\n",
       "      <td>1,478,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>11/25/2016</td>\n",
       "      <td>764.26</td>\n",
       "      <td>765.00</td>\n",
       "      <td>760.52</td>\n",
       "      <td>761.68</td>\n",
       "      <td>587,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>11/28/2016</td>\n",
       "      <td>760.00</td>\n",
       "      <td>779.53</td>\n",
       "      <td>759.80</td>\n",
       "      <td>768.24</td>\n",
       "      <td>2,188,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>11/29/2016</td>\n",
       "      <td>771.53</td>\n",
       "      <td>778.50</td>\n",
       "      <td>768.24</td>\n",
       "      <td>770.84</td>\n",
       "      <td>1,616,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>11/30/2016</td>\n",
       "      <td>770.07</td>\n",
       "      <td>772.99</td>\n",
       "      <td>754.83</td>\n",
       "      <td>758.04</td>\n",
       "      <td>2,392,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>12/1/2016</td>\n",
       "      <td>757.44</td>\n",
       "      <td>759.85</td>\n",
       "      <td>737.03</td>\n",
       "      <td>747.92</td>\n",
       "      <td>3,017,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>12/2/2016</td>\n",
       "      <td>744.59</td>\n",
       "      <td>754.00</td>\n",
       "      <td>743.10</td>\n",
       "      <td>750.5</td>\n",
       "      <td>1,452,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>12/5/2016</td>\n",
       "      <td>757.71</td>\n",
       "      <td>763.90</td>\n",
       "      <td>752.90</td>\n",
       "      <td>762.52</td>\n",
       "      <td>1,394,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>12/6/2016</td>\n",
       "      <td>764.73</td>\n",
       "      <td>768.83</td>\n",
       "      <td>757.34</td>\n",
       "      <td>759.11</td>\n",
       "      <td>1,690,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>12/7/2016</td>\n",
       "      <td>761.00</td>\n",
       "      <td>771.36</td>\n",
       "      <td>755.80</td>\n",
       "      <td>771.19</td>\n",
       "      <td>1,761,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>12/8/2016</td>\n",
       "      <td>772.48</td>\n",
       "      <td>778.18</td>\n",
       "      <td>767.23</td>\n",
       "      <td>776.42</td>\n",
       "      <td>1,488,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>12/9/2016</td>\n",
       "      <td>780.00</td>\n",
       "      <td>789.43</td>\n",
       "      <td>779.02</td>\n",
       "      <td>789.29</td>\n",
       "      <td>1,821,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>12/12/2016</td>\n",
       "      <td>785.04</td>\n",
       "      <td>791.25</td>\n",
       "      <td>784.35</td>\n",
       "      <td>789.27</td>\n",
       "      <td>2,104,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>12/13/2016</td>\n",
       "      <td>793.90</td>\n",
       "      <td>804.38</td>\n",
       "      <td>793.34</td>\n",
       "      <td>796.1</td>\n",
       "      <td>2,145,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>12/14/2016</td>\n",
       "      <td>797.40</td>\n",
       "      <td>804.00</td>\n",
       "      <td>794.01</td>\n",
       "      <td>797.07</td>\n",
       "      <td>1,704,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>12/15/2016</td>\n",
       "      <td>797.34</td>\n",
       "      <td>803.00</td>\n",
       "      <td>792.92</td>\n",
       "      <td>797.85</td>\n",
       "      <td>1,626,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>12/16/2016</td>\n",
       "      <td>800.40</td>\n",
       "      <td>800.86</td>\n",
       "      <td>790.29</td>\n",
       "      <td>790.8</td>\n",
       "      <td>2,443,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>12/19/2016</td>\n",
       "      <td>790.22</td>\n",
       "      <td>797.66</td>\n",
       "      <td>786.27</td>\n",
       "      <td>794.2</td>\n",
       "      <td>1,232,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>12/20/2016</td>\n",
       "      <td>796.76</td>\n",
       "      <td>798.65</td>\n",
       "      <td>793.27</td>\n",
       "      <td>796.42</td>\n",
       "      <td>951,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>12/21/2016</td>\n",
       "      <td>795.84</td>\n",
       "      <td>796.68</td>\n",
       "      <td>787.10</td>\n",
       "      <td>794.56</td>\n",
       "      <td>1,211,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>12/22/2016</td>\n",
       "      <td>792.36</td>\n",
       "      <td>793.32</td>\n",
       "      <td>788.58</td>\n",
       "      <td>791.26</td>\n",
       "      <td>972,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close      Volume\n",
       "0       1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1       1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2       1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3       1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4       1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
       "5      1/10/2012  313.70  315.72  307.30  621.43   8,824,000\n",
       "6      1/11/2012  310.59  313.52  309.40  624.25   4,817,800\n",
       "7      1/12/2012  314.43  315.26  312.08  627.92   3,764,400\n",
       "8      1/13/2012  311.96  312.30  309.37  623.28   4,631,800\n",
       "9      1/17/2012  314.81  314.81  311.67  626.86   3,832,800\n",
       "10     1/18/2012  312.14  315.82  309.90  631.18   5,544,000\n",
       "11     1/19/2012  319.30  319.30  314.55  637.82  12,657,800\n",
       "12     1/20/2012  294.16  294.40  289.76  584.39  21,231,800\n",
       "13     1/23/2012  291.91  293.23  290.49  583.92   6,851,300\n",
       "14     1/24/2012  292.07  292.74  287.92  579.34   6,134,400\n",
       "15     1/25/2012  287.68  288.27  282.13  567.93  10,012,700\n",
       "16     1/26/2012  284.92  286.17  281.22  566.54   6,476,500\n",
       "17     1/27/2012  284.32  289.08  283.60  578.39   7,262,000\n",
       "18     1/30/2012  287.95  288.92  285.63  576.11   4,678,400\n",
       "19     1/31/2012  290.41  290.91  286.50  578.52   4,300,700\n",
       "20      2/1/2012  291.38  291.66  288.49  579.24   4,658,700\n",
       "21      2/2/2012  291.34  292.11  289.95  583.51   4,847,400\n",
       "22      2/3/2012  294.23  297.42  292.93   594.7   6,360,700\n",
       "23      2/6/2012  296.39  304.27  295.90  607.42   7,386,700\n",
       "24      2/7/2012  302.44  303.56  300.75  605.11   4,199,700\n",
       "25      2/8/2012  303.18  304.53  301.24  608.18   3,686,400\n",
       "26      2/9/2012  304.87  306.10  303.36  609.79   4,546,300\n",
       "27     2/10/2012  302.81  302.93  300.87  604.25   4,667,700\n",
       "28     2/13/2012  304.11  305.77  303.87  610.52   3,646,100\n",
       "29     2/14/2012  304.63  304.86  301.25  608.09   3,620,900\n",
       "...          ...     ...     ...     ...     ...         ...\n",
       "1228  11/17/2016  766.92  772.70  764.23  771.23   1,304,000\n",
       "1229  11/18/2016  771.37  775.00  760.00  760.54   1,547,100\n",
       "1230  11/21/2016  762.61  769.70  760.60   769.2   1,330,600\n",
       "1231  11/22/2016  772.63  776.96  767.00  768.27   1,593,100\n",
       "1232  11/23/2016  767.73  768.28  755.25  760.99   1,478,400\n",
       "1233  11/25/2016  764.26  765.00  760.52  761.68     587,400\n",
       "1234  11/28/2016  760.00  779.53  759.80  768.24   2,188,200\n",
       "1235  11/29/2016  771.53  778.50  768.24  770.84   1,616,600\n",
       "1236  11/30/2016  770.07  772.99  754.83  758.04   2,392,900\n",
       "1237   12/1/2016  757.44  759.85  737.03  747.92   3,017,900\n",
       "1238   12/2/2016  744.59  754.00  743.10   750.5   1,452,500\n",
       "1239   12/5/2016  757.71  763.90  752.90  762.52   1,394,200\n",
       "1240   12/6/2016  764.73  768.83  757.34  759.11   1,690,700\n",
       "1241   12/7/2016  761.00  771.36  755.80  771.19   1,761,000\n",
       "1242   12/8/2016  772.48  778.18  767.23  776.42   1,488,100\n",
       "1243   12/9/2016  780.00  789.43  779.02  789.29   1,821,900\n",
       "1244  12/12/2016  785.04  791.25  784.35  789.27   2,104,100\n",
       "1245  12/13/2016  793.90  804.38  793.34   796.1   2,145,200\n",
       "1246  12/14/2016  797.40  804.00  794.01  797.07   1,704,200\n",
       "1247  12/15/2016  797.34  803.00  792.92  797.85   1,626,500\n",
       "1248  12/16/2016  800.40  800.86  790.29   790.8   2,443,800\n",
       "1249  12/19/2016  790.22  797.66  786.27   794.2   1,232,100\n",
       "1250  12/20/2016  796.76  798.65  793.27  796.42     951,000\n",
       "1251  12/21/2016  795.84  796.68  787.10  794.56   1,211,300\n",
       "1252  12/22/2016  792.36  793.32  788.58  791.26     972,200\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91     623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55     789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05   1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79     744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82   1,770,000\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "#     Its recommended to use NORMALIZATION in RNN when we have sigmoid as our activation function for the\n",
    "#     output layer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0,1))\n",
    "#     Recommended to keep training set unaltered\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "#     60 timesteps means that at each time t the RNN is going to look at 60 stock prices\n",
    "#     i.e stock prices between 60 days before time t and time t\n",
    "#     Based on the trends it will predict the output at time t+1\n",
    "\n",
    "X_train = []  #Contains 60 stock prices before that financial day\n",
    "y_train = []  #Contain the stock price of the next financial day\n",
    "\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60: i, 0]) #we've only 1 columns(but : makes it 3d) so we use 0\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "\n",
    "#     X_train and y_train are LISTS but we need arrays\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368, 0.09701243, 0.09433366, ..., 0.07846566, 0.08034452,\n",
       "        0.08497656],\n",
       "       [0.09701243, 0.09433366, 0.09156187, ..., 0.08034452, 0.08497656,\n",
       "        0.08627874],\n",
       "       [0.09433366, 0.09156187, 0.07984225, ..., 0.08497656, 0.08627874,\n",
       "        0.08471612],\n",
       "       ...,\n",
       "       [0.92106928, 0.92438053, 0.93048218, ..., 0.95475854, 0.95204256,\n",
       "        0.95163331],\n",
       "       [0.92438053, 0.93048218, 0.9299055 , ..., 0.95204256, 0.95163331,\n",
       "        0.95725128],\n",
       "       [0.93048218, 0.9299055 , 0.93113327, ..., 0.95163331, 0.95725128,\n",
       "        0.93796041]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08627874, 0.08471612, 0.07454052, ..., 0.95725128, 0.93796041,\n",
       "       0.93688146])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "#     Adding some more dimensionality to the data structure\n",
    "#     It is the unit that is the number of predictors we can use to predict what we want\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) # 1 indicates the no. of indicators\n",
    "\n",
    "#   This is imp if we want to add some more indicators or do some more robust financial engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.08581368],\n",
       "        [0.09701243],\n",
       "        [0.09433366],\n",
       "        ...,\n",
       "        [0.07846566],\n",
       "        [0.08034452],\n",
       "        [0.08497656]],\n",
       "\n",
       "       [[0.09701243],\n",
       "        [0.09433366],\n",
       "        [0.09156187],\n",
       "        ...,\n",
       "        [0.08034452],\n",
       "        [0.08497656],\n",
       "        [0.08627874]],\n",
       "\n",
       "       [[0.09433366],\n",
       "        [0.09156187],\n",
       "        [0.07984225],\n",
       "        ...,\n",
       "        [0.08497656],\n",
       "        [0.08627874],\n",
       "        [0.08471612]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.92106928],\n",
       "        [0.92438053],\n",
       "        [0.93048218],\n",
       "        ...,\n",
       "        [0.95475854],\n",
       "        [0.95204256],\n",
       "        [0.95163331]],\n",
       "\n",
       "       [[0.92438053],\n",
       "        [0.93048218],\n",
       "        [0.9299055 ],\n",
       "        ...,\n",
       "        [0.95204256],\n",
       "        [0.95163331],\n",
       "        [0.95725128]],\n",
       "\n",
       "       [[0.93048218],\n",
       "        [0.9299055 ],\n",
       "        [0.93113327],\n",
       "        ...,\n",
       "        [0.95163331],\n",
       "        [0.95725128],\n",
       "        [0.93796041]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amit_bahir/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/amit_bahir/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#  ***Part 2 -> Building the RNN***\n",
    "#     We're going to build a stacked LSTM with some dropout regularization to prevent overfitting\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Importing the keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# Initializing the RNN as a sequence of layers\n",
    "#     We will use computational graphs and build them using PyTorch later\n",
    "\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the 1st LSTM layer and some Dropout regularization\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1))) # contains the last 2 dims\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the 2nd LSTM layer and some Dropout regularization\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the 3rd LSTM layer and some Dropout regularization\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the 4th LSTM layer and some Dropout regularization\n",
    "regressor.add(LSTM(units = 50)) #its by default false\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer= 'rmsprop', loss = 'mean_squared_error')\n",
    "# regressor.compile(optimizer= 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amit_bahir/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 5s 5ms/step - loss: 0.0178\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0062\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0047\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0042\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 3s 2ms/step - loss: 0.0039\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 3s 2ms/step - loss: 0.0034\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0033\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0034\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0027\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0026\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 3s 2ms/step - loss: 0.0028\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0023\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0023\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0023\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0022\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0022\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0021\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0022\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0019\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0021\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 3s 2ms/step - loss: 0.0019\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0017\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0018\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 3s 2ms/step - loss: 0.0017\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0017\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 3s 2ms/step - loss: 0.0017\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0016\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0015\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0015\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0015\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0014\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0013\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0014\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0013\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0015\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0013\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0013\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0013\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0014\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0013\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0013\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0012\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0010\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0011\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0010\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0010\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.7551e-04\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.4428e-04\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.6418e-04\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.4360e-04\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.7669e-04\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0010\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 0.0010\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.9971e-04\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.9208e-04\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.0935e-04\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 8.9606e-04\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.8992e-04\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.4502e-04\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 9.0871e-04\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.1443e-04\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.7000e-04\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.9870e-04\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.7284e-04\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 8.5481e-04\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 9.5633e-04\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.4480e-04\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.9424e-04\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.1405e-04\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.0200e-04\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.8819e-04\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 7.8639e-04\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.1912e-04\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.8513e-04\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.5079e-04\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.4106e-04\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.4632e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 9.1877e-04\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.1141e-04\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 9.1300e-04\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.3079e-04\n",
      "Epoch 97/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.3275e-04\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.3895e-04\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 8.1278e-04\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 3s 3ms/step - loss: 7.8502e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f231b6783c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the training set\n",
    "regressor.fit(X_train, y_train, epochs= 100, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***Part 3 -> Predicting and Visualizing the results***\n",
    "\n",
    "#     Getting the real stock price of 2017\n",
    "df_test = pd.read_csv('Recurrent_Neural_Networks/Google_Stock_Price_Test.csv')\n",
    "real_stock_price = df_test.iloc[:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(real_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Getting the predicted stock price of 2017\n",
    "#     Now for getting the stock prices of previous 60 days, we'll need some days from Jan 2017 \n",
    "#     and some from Nov,Dec 2016\n",
    "#     So for that we need to concatenate the two datasets\n",
    "\n",
    "#     We will need to scale the test set but the problem is that we will change the actual test set and we should \n",
    "#     never do this\n",
    "\n",
    "#     So, to overcome this we'll concatenate the original dataframes and then scale only the inputs and \n",
    "#     not the actual test values\n",
    "df_total = pd.concat((df_train['Open'], df_test['Open']), axis= 0)\n",
    "inputs = df_total[len(df_total) - len(df_test) - 60:].values\n",
    "inputs = inputs.reshape(-1, 1)\n",
    "inputs = sc.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "# Reshaping\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Predictiing\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[765.40155],\n",
       "       [763.6898 ],\n",
       "       [766.2755 ],\n",
       "       [766.4912 ],\n",
       "       [768.49   ],\n",
       "       [772.865  ],\n",
       "       [774.9013 ],\n",
       "       [774.3995 ],\n",
       "       [775.2225 ],\n",
       "       [776.6919 ],\n",
       "       [777.56714],\n",
       "       [777.6972 ],\n",
       "       [777.79095],\n",
       "       [778.64514],\n",
       "       [779.3105 ],\n",
       "       [783.3361 ],\n",
       "       [786.8103 ],\n",
       "       [789.5053 ],\n",
       "       [790.1139 ],\n",
       "       [786.5736 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the Results\n",
    "#     Witness the robustness of our model\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'powderblue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
